{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1658933,"sourceType":"datasetVersion","datasetId":982120}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-24T13:21:34.159539Z","iopub.execute_input":"2023-12-24T13:21:34.160249Z","iopub.status.idle":"2023-12-24T13:21:34.588338Z","shell.execute_reply.started":"2023-12-24T13:21:34.160199Z","shell.execute_reply":"2023-12-24T13:21:34.587117Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.nn import functional as F\nfrom torch.utils.data import Dataset,DataLoader\nimport collections \nimport re","metadata":{"execution":{"iopub.status.busy":"2023-12-24T14:19:07.180992Z","iopub.execute_input":"2023-12-24T14:19:07.181448Z","iopub.status.idle":"2023-12-24T14:19:07.187911Z","shell.execute_reply.started":"2023-12-24T14:19:07.181414Z","shell.execute_reply":"2023-12-24T14:19:07.186326Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class Vocab:\n    def __init__(self, tokens=[], min_freq=0, reserved_tokens=[]):\n        #count token frequencies\n        counter = collections.Counter(tokens)\n        self.token_freqs = sorted(counter.items(), key=lambda x:x[1], reverse=True)\n        \n        #list of unique tokens\n        self.itos = list(sorted(set(['<unk>'] + reserved_tokens + [token for token, freq in self.token_freqs if freq >= min_freq])))\n        self.stoi = {token: idx for idx,token in enumerate(self.itos) }\n        \n    def __len__(self):\n        #length of vocabulary\n        return len(self.itos)\n    \n    def __getitem__(self, tokens):\n        #make tokens into indices\n        if not isinstance(tokens,(list,tuple)):\n            return self.stoi[tokens]\n        return [self.__getitem__(token) for token in tokens]\n    \n    def to_tokens(self,indices):\n        #make indixes into tokens\n        if hasattr(indices,'__len__') and len(indices)>1:\n            return [self.itos[int(index)] for index in indices]\n        return slef.itos[indices]\n    \n    @property\n    def unk(self):\n        return self.token_to_idx['<unk>']\nclass text_dataset(Dataset):\n    def __init__(self, num_steps,train= True, train_size=10000, val_size=5000):\n        super().__init__()\n        corpus,self.vocab = self.build(self._load())\n        array = torch.tensor([corpus[i:i+num_steps+1] for i in range(len(corpus)-num_steps)])\n        if train:\n            self.X,self.Y = array[:,:-1][:train_size],array[:,1:][:train_size]\n        else:\n            self.X,self.Y = array[:,:-1][train_size:train_size+val_size],array[:,1:][train_size:train_size+val_size]\n        \n    def _load(self,path= '/kaggle/input/time-machine/timemachine.txt'):\n        with open(path,\"r\") as f:\n            return f.read()\n        \n    def _preprocess(self, raw_text):\n        return re.sub('[^A-Za-z]+',' ',raw_text).lower()\n    \n    def _tokenize(self,text):\n        return list(text)\n    \n    def build(self,raw_text,vocab=None):\n        tokens = self._tokenize(self._preprocess(raw_text))\n        if vocab is None: vocab = Vocab(tokens)\n        corpus = [vocab[token] for token in tokens]\n        return corpus, vocab\n    \n    def __getitem__(self,index):\n        sample = self.X[index],self.Y[index]\n        return sample\n    \n    def __len__(self):\n        return self.X.shape[0]","metadata":{"execution":{"iopub.status.busy":"2023-12-24T14:47:30.165398Z","iopub.execute_input":"2023-12-24T14:47:30.165837Z","iopub.status.idle":"2023-12-24T14:47:30.186931Z","shell.execute_reply.started":"2023-12-24T14:47:30.165801Z","shell.execute_reply":"2023-12-24T14:47:30.185473Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"class RNN(nn.Module):\n    def __init__(self,input_size,hidden_size):\n        super().__init__()\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.rnn = nn.RNN(input_size, hidden_size)\n    \n    def forward(self,inputs,H=None):\n        return self.rnn(inputs, H)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T14:24:12.819624Z","iopub.execute_input":"2023-12-24T14:24:12.820333Z","iopub.status.idle":"2023-12-24T14:24:12.830148Z","shell.execute_reply.started":"2023-12-24T14:24:12.820277Z","shell.execute_reply":"2023-12-24T14:24:12.828191Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"see docs for torch.RNN\\\noutput shape:T,N,channel_len","metadata":{}},{"cell_type":"code","source":"class RNNLM(nn.Module):\n    '''language model based on RNN'''\n    def __init__(self, rnn, vocab_size, clip_value=100.0):\n        super().__init__()\n        self.rnn = rnn\n        self.vocab_size = vocab_size\n        self.clip_value = clip_value\n#         self.lr = lr\n#         self.W_hq = nn.Parameter(torch.randn(self.rnn.hidden_size,vocab_size)*self.rnn.sigma)\n#     #the output size set to be vocab_size\n#         self.b_q = nn.Parameter(torch.zeros(self.vocab_size))\n        self.linear = nn.Linear(self.rnn.hidden_size,vocab_size)\n        \n    def one_hot(self,X):\n        return F.one_hot(X.T, self.vocab_size).type(torch.float32)\n    \n    def output_layer(self,hiddens):\n#         outputs = [torch.matmul(H,self.W_hq) + self.b_q for H in rnn_outputs]\n#         return torch.stack(outputs, 1)#num_steps,batch_size,vocab_size\n        return self.linear(hiddens).transpose(0,1)\n#passing through the linear it is T,B,C(like the input) after that (B,C,T)\n    def forward(self,X,state=None):\n        '''\n        X:batch_size,time_step\n        '''\n        embs = self.one_hot(X)\n        # embs.shape:timesteps,batch_size,embedding_dimension\n        rnn_outputs,_ = self.rnn(embs,state)\n        return self.output_layer(rnn_outputs)\n        \n    def clip_gradient(self):\n        norm = 0\n        for parameter in self.parameters():\n            if parameter.requires_grad:\n                norm += torch.sum(parameter.grad**2)\n            norm = torch.sqrt(norm)\n        if norm > self.clip_value:\n            for param in self.parameters():\n                param.grad*=self.clip_value/norm\n    def generate(self,prefix,num_preds,vocab,device = \"cuda\" if torch.cuda.is_available() else \"cpu\"):\n        state,outputs = None,[vocab[prefix[0]]]#2D array that batch_size,1(index of prefix[0])\n        for i in range(len(prefix) + num_preds - 1):\n            X = torch.tensor([[outputs[-1]]],device=device)\n            embs = self.one_hot(X)\n            rnn_outputs, state = self.rnn(embs,state)\n            if i < len(prefix) - 1:\n                outputs.append(vocab[prefix[i+1]])\n            else:\n                Y = self.output_layer(rnn_outputs)\n                outputs.append(int(Y.argmax(axis=2).reshape(1)))\n        return ''.join([vocab.itos[i] for i in outputs])","metadata":{"execution":{"iopub.status.busy":"2023-12-24T14:54:38.303270Z","iopub.execute_input":"2023-12-24T14:54:38.303780Z","iopub.status.idle":"2023-12-24T14:54:38.319632Z","shell.execute_reply.started":"2023-12-24T14:54:38.303748Z","shell.execute_reply":"2023-12-24T14:54:38.318333Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"train_data = text_dataset(num_steps=32,train=True)\ntest_data = text_dataset(num_steps=32,train=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T14:54:39.141703Z","iopub.execute_input":"2023-12-24T14:54:39.142569Z","iopub.status.idle":"2023-12-24T14:54:43.678873Z","shell.execute_reply.started":"2023-12-24T14:54:39.142516Z","shell.execute_reply":"2023-12-24T14:54:43.677497Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"rnn = RNN(input_size=len(train_data.vocab),hidden_size=32)\nmodel = RNNLM(rnn,vocab_size=len(train_data.vocab) )#this decodes to a size same as the input","metadata":{"execution":{"iopub.status.busy":"2023-12-24T14:54:43.681268Z","iopub.execute_input":"2023-12-24T14:54:43.682246Z","iopub.status.idle":"2023-12-24T14:54:43.689485Z","shell.execute_reply.started":"2023-12-24T14:54:43.682198Z","shell.execute_reply":"2023-12-24T14:54:43.688241Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"for name,x in model.named_parameters():\n    print(name,i)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T14:54:43.691107Z","iopub.execute_input":"2023-12-24T14:54:43.691526Z","iopub.status.idle":"2023-12-24T14:54:43.701491Z","shell.execute_reply.started":"2023-12-24T14:54:43.691470Z","shell.execute_reply":"2023-12-24T14:54:43.700237Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stdout","text":"rnn.rnn.weight_ih_l0 9\nrnn.rnn.weight_hh_l0 9\nrnn.rnn.bias_ih_l0 9\nrnn.rnn.bias_hh_l0 9\nlinear.weight 9\nlinear.bias 9\n","output_type":"stream"}]},{"cell_type":"markdown","source":"ih and hh biases are different","metadata":{}},{"cell_type":"code","source":"lr = 1\nnum_epochs = 100\noptimizer = torch.optim.AdamW(model.parameters(),lr=lr)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T14:54:45.342845Z","iopub.execute_input":"2023-12-24T14:54:45.343319Z","iopub.status.idle":"2023-12-24T14:54:45.349031Z","shell.execute_reply.started":"2023-12-24T14:54:45.343282Z","shell.execute_reply":"2023-12-24T14:54:45.347838Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"train_loader = DataLoader(dataset=train_data,batch_size=1024,shuffle=True)\ntest_loader = DataLoader(dataset=test_data,batch_size=1024,shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T14:54:46.886517Z","iopub.execute_input":"2023-12-24T14:54:46.887491Z","iopub.status.idle":"2023-12-24T14:54:46.900439Z","shell.execute_reply.started":"2023-12-24T14:54:46.887440Z","shell.execute_reply":"2023-12-24T14:54:46.899182Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"for epoch in range(num_epochs):    \n    for i,(sample,targets) in enumerate(train_loader):\n        out = model(sample)\n#         out.shape:num_steps,batch_size,channel/vocab\n#         print(sample.shape)\n        batch_size,num_steps,vocab_size = out.shape\n#         if i==0:\n#             print(out.shape,targets.shape)\n#             break\n#         print(out.shape)\n        out = out.reshape(num_steps*batch_size,-1)\n        targets = targets.view(num_steps*batch_size,)\n        \n        loss = F.cross_entropy(out,targets)\n        optimizer.zero_grad()\n        loss.backward()\n#         model.clip_gradient()\n        optimizer.step() ","metadata":{"execution":{"iopub.status.busy":"2023-12-24T14:56:23.664053Z","iopub.execute_input":"2023-12-24T14:56:23.664487Z","iopub.status.idle":"2023-12-24T14:57:00.283191Z","shell.execute_reply.started":"2023-12-24T14:56:23.664453Z","shell.execute_reply":"2023-12-24T14:57:00.282206Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"loss.item()\nmodel.generate('father is using mobile', 20, train_data.vocab)","metadata":{"execution":{"iopub.status.busy":"2023-12-24T14:57:00.284768Z","iopub.execute_input":"2023-12-24T14:57:00.285387Z","iopub.status.idle":"2023-12-24T14:57:00.301391Z","shell.execute_reply.started":"2023-12-24T14:57:00.285353Z","shell.execute_reply":"2023-12-24T14:57:00.300450Z"},"trusted":true},"execution_count":83,"outputs":[{"execution_count":83,"output_type":"execute_result","data":{"text/plain":"'father is using mobile the the the the the'"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}